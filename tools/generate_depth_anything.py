#!/usr/bin/env python3
"""
ä½¿ç”¨Depth Anythingç”Ÿæˆdepthå›¾åƒ

è¾“å…¥ï¼šRGBå›¾åƒè·¯å¾„
è¾“å‡ºï¼šdepthå›¾åƒï¼ˆPNGæ ¼å¼ï¼Œuint16ï¼‰
"""
import argparse
import sys
from pathlib import Path
import torch
import numpy as np
from PIL import Image
from tqdm import tqdm

try:
    from depth_anything_v2.dpt import DepthAnythingV2
    HAS_DEPTH_ANYTHING = True
except ImportError:
    HAS_DEPTH_ANYTHING = False
    print("âš ï¸  depth_anything_v2æœªå®‰è£…ï¼Œå°†å°è¯•ä½¿ç”¨transformersåº“")

def load_depth_anything_model(device='cuda'):
    """åŠ è½½Depth Anythingæ¨¡å‹"""
    if HAS_DEPTH_ANYTHING:
        model_configs = {
            'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},
            'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},
            'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [192, 384, 768, 1536]}
        }
        model = DepthAnythingV2(**model_configs['vitl'])
        model.load_state_dict(torch.load('checkpoints/depth_anything_v2_vitl.pth', map_location='cpu'))
        model.to(device).eval()
        return model
    else:
        # å°è¯•ä½¿ç”¨transformersåº“
        try:
            from transformers import AutoImageProcessor, AutoModelForDepthEstimation
            processor = AutoImageProcessor.from_pretrained("LiheYoung/depth-anything-v2-base-hf")
            model = AutoModelForDepthEstimation.from_pretrained("LiheYoung/depth-anything-v2-base-hf")
            model.to(device).eval()
            return (processor, model)
        except Exception as e:
            print(f"âŒ æ— æ³•åŠ è½½Depth Anythingæ¨¡å‹: {e}")
            return None

def generate_depth_with_depth_anything(image_path: Path, output_path: Path, model, device='cuda'):
    """ä½¿ç”¨Depth Anythingç”Ÿæˆdepthå›¾åƒ"""
    try:
        # åŠ è½½RGBå›¾åƒ
        image = Image.open(image_path).convert('RGB')
        image_np = np.array(image)
        
        if HAS_DEPTH_ANYTHING:
            # ä½¿ç”¨depth_anything_v2åº“
            depth = model.infer_image(image_np)
            # è½¬æ¢ä¸ºuint16ï¼ˆå•ä½ï¼šmmï¼ŒèŒƒå›´0-65535ï¼‰
            depth_uint16 = (depth * 1000).clip(0, 65535).astype(np.uint16)
        else:
            # ä½¿ç”¨transformersåº“
            processor, model_obj = model
            inputs = processor(images=image, return_tensors="pt").to(device)
            with torch.no_grad():
                outputs = model_obj(**inputs)
                predicted_depth = outputs.predicted_depth
            
            # è½¬æ¢ä¸ºnumpyå¹¶è°ƒæ•´å°ºå¯¸
            depth = predicted_depth.cpu().numpy()[0, 0]
            # è°ƒæ•´åˆ°åŸå›¾å°ºå¯¸
            from scipy.ndimage import zoom
            h, w = image_np.shape[:2]
            depth = zoom(depth, (h / depth.shape[0], w / depth.shape[1]))
            # è½¬æ¢ä¸ºuint16ï¼ˆå•ä½ï¼šmmï¼‰
            depth_uint16 = (depth * 1000).clip(0, 65535).astype(np.uint16)
        
        # ä¿å­˜depthå›¾åƒ
        depth_img = Image.fromarray(depth_uint16, mode='I;16')
        output_path.parent.mkdir(parents=True, exist_ok=True)
        depth_img.save(output_path)
        return True
    except Exception as e:
        print(f"âš ï¸  ç”Ÿæˆdepthå¤±è´¥ {image_path}: {e}")
        return False

def generate_depths_for_five_frames(five_frames_root: Path, depth_output_root: Path, device='cuda'):
    """ä¸ºfive_framesæ•°æ®ç”Ÿæˆdepthå›¾åƒ"""
    print(f"ğŸ“‚ æ‰«æfive_framesæ•°æ®: {five_frames_root}")
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ”§ åŠ è½½Depth Anythingæ¨¡å‹...")
    model = load_depth_anything_model(device)
    if model is None:
        print("âŒ æ— æ³•åŠ è½½Depth Anythingæ¨¡å‹")
        return False
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # æ‰«ææ‰€æœ‰å›¾åƒ
    image_files = []
    for split in ["train", "validation"]:
        split_dir = five_frames_root / split
        if not split_dir.exists():
            continue
        
        for scene_dir in split_dir.iterdir():
            if not scene_dir.is_dir():
                continue
            
            for uid_dir in scene_dir.iterdir():
                if not uid_dir.is_dir() or not uid_dir.name.startswith("uid_"):
                    continue
                
                # æŸ¥æ‰¾æ‰€æœ‰originalå›¾åƒ
                for img_file in uid_dir.glob("*_original.png"):
                    image_files.append(img_file)
    
    print(f"âœ… æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒ")
    
    # ç”Ÿæˆdepth
    success_count = 0
    for img_path in tqdm(image_files, desc="ç”Ÿæˆdepth"):
        # æ„é€ depthè¾“å‡ºè·¯å¾„
        # ä¾‹å¦‚: train/scene/uid_xxx/01_001640_original.png -> depth_output/train/scene/uid_xxx/01_001640_depth.png
        rel_path = img_path.relative_to(five_frames_root)
        depth_path = depth_output_root / rel_path.parent / rel_path.name.replace("_original.png", "_depth.png")
        
        # å¦‚æœå·²å­˜åœ¨ï¼Œè·³è¿‡
        if depth_path.exists():
            continue
        
        if generate_depth_with_depth_anything(img_path, depth_path, model, device):
            success_count += 1
    
    print(f"\nâœ… å®Œæˆï¼æˆåŠŸç”Ÿæˆ {success_count}/{len(image_files)} å¼ depthå›¾åƒ")
    return True

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--five_frames_root", type=str, required=True,
                       help="five_framesæ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--depth_output_root", type=str, required=True,
                       help="depthè¾“å‡ºæ ¹ç›®å½•")
    parser.add_argument("--device", type=str, default="cuda",
                       help="è®¾å¤‡ (cuda/cpu)")
    
    args = parser.parse_args()
    
    generate_depths_for_five_frames(
        Path(args.five_frames_root),
        Path(args.depth_output_root),
        args.device
    )
