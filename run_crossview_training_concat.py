#!/usr/bin/env python3
"""
è·¨è§†è§’ä¸€è‡´æ€§è®­ç»ƒè„šæœ¬ï¼ˆä½¿ç”¨æ‹¼æ¥å›¾æ ¼å¼ï¼ŒåªåŒ…å«æ­£ä¾‹ï¼‰
"""
import subprocess
import os
import sys
from pathlib import Path

# è®¾ç½®å·¥ä½œç›®å½•
os.chdir(Path(__file__).parent)
os.environ['PYTHONPATH'] = str(Path.cwd())

# æ£€æŸ¥torchæ˜¯å¦å¯ç”¨
def check_torch():
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        return True
    except ImportError:
        print("âŒ å½“å‰ç¯å¢ƒæ²¡æœ‰å®‰è£…PyTorch")
        print("è¯·å…ˆæ¿€æ´»condaç¯å¢ƒ: conda activate roborefer")
        return False

# ==================== è®­ç»ƒé…ç½® ====================
# Baseæ¨¡å‹
base_model = "./runs/train/RoboRefer-2B-Depth-Align"

# é€‰æ‹©æ•°æ®ç‰ˆæœ¬ï¼šoriginal æˆ– x3
# - original: ä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆæœªé™é‡‡æ ·ï¼‰
# - x3: ä½¿ç”¨é™é‡‡æ ·æ•°æ®ï¼ˆ680x440ï¼Œé™é‡‡æ ·3å€ï¼‰
DATA_VERSION = os.environ.get("DATA_VERSION", "x3")  # é»˜è®¤ä½¿ç”¨x3

if DATA_VERSION == "original":
    data_mixture = "crossview_concat_corrected_original"
    output_dir = "runs/train/RoboRefer-2B-CrossView-Concat-Original"
    print("ğŸ“‚ ä½¿ç”¨åŸå§‹æ•°æ®æ‹¼æ¥å›¾ï¼ˆæœªé™é‡‡æ ·ï¼‰")
elif DATA_VERSION == "x3":
    data_mixture = "crossview_concat_corrected_x3"
    output_dir = "runs/train/RoboRefer-2B-CrossView-Concat-X3"
    print("ğŸ“‚ ä½¿ç”¨é™é‡‡æ ·æ•°æ®æ‹¼æ¥å›¾ï¼ˆ680x440ï¼‰")
else:
    print(f"âŒ é”™è¯¯: DATA_VERSIONå¿…é¡»æ˜¯ 'original' æˆ– 'x3'ï¼Œå½“å‰ä¸º: {DATA_VERSION}")
    print("   è®¾ç½®æ–¹å¼: export DATA_VERSION=original æˆ– export DATA_VERSION=x3")
    sys.exit(1)

# æ£€æŸ¥base model
if not os.path.exists(base_model):
    print(f"âŒ Base modelä¸å­˜åœ¨: {base_model}")
    sys.exit(1)

print("="*70)
print("ğŸš€ è·¨è§†è§’ä¸€è‡´æ€§è®­ç»ƒï¼ˆæ‹¼æ¥å›¾æ ¼å¼ï¼ŒåªåŒ…å«æ­£ä¾‹ï¼‰")
print("="*70)
print(f"âœ… Base model: {base_model}")
print(f"âœ… æ•°æ®é›†: {data_mixture}")
print(f"âœ… è¾“å‡ºç›®å½•: {output_dir}")
print(f"\nğŸ“‹ è®­ç»ƒé…ç½®:")
print(f"   - æ•°æ®æ ¼å¼: æ‹¼æ¥å›¾ï¼ˆå•å¼ å›¾åƒï¼‰")
print(f"   - æ ·æœ¬ç±»å‹: åªåŒ…å«æ­£ä¾‹ï¼Œä¸åŒ…å«è´Ÿä¾‹")
print(f"   - Batch size: 1 per device")
print(f"   - Gradient accumulation: 4 steps")
print(f"   - Effective batch size: 4")
print(f"   - Epochs: 2")
print(f"   - Learning rate: 1e-5")
print(f"   - Image aspect ratio: dynamic")
print(f"   - è¾“å‡ºæ ¼å¼: [(x, y)]")
print("="*70)
print()

# åˆ›å»ºè¾“å‡ºç›®å½•
Path(f"{output_dir}/model").mkdir(parents=True, exist_ok=True)

# æ£€æŸ¥ç¯å¢ƒ
if not check_torch():
    sys.exit(1)

# æ£€æŸ¥å¯ç”¨çš„GPU
try:
    gpu_info = subprocess.run(
        ["nvidia-smi", "--query-gpu=index,memory.used,memory.total", "--format=csv,noheader,nounits"],
        capture_output=True,
        text=True
    )
    if gpu_info.returncode == 0:
        gpus = []
        for line in gpu_info.stdout.strip().split('\n'):
            if line:
                parts = line.split(', ')
                gpu_idx = int(parts[0])
                mem_used = int(parts[1])
                mem_total = int(parts[2])
                mem_free = mem_total - mem_used
                gpus.append((gpu_idx, mem_free, mem_total))
        
        # é€‰æ‹©æ˜¾å­˜æœ€å¤šçš„GPU
        gpus.sort(key=lambda x: x[1], reverse=True)
        best_gpu = gpus[0][0]
        print(f"âœ… é€‰æ‹©GPU {best_gpu} (å¯ç”¨æ˜¾å­˜: {gpus[0][1]}MB / {gpus[0][2]}MB)")
        os.environ['CUDA_VISIBLE_DEVICES'] = str(best_gpu)
except Exception as e:
    print(f"âš ï¸  GPUé€‰æ‹©å¤±è´¥: {e}")

# ==================== æ„å»ºè®­ç»ƒå‘½ä»¤ ====================
cmd = [
    sys.executable, "-m", "torch.distributed.run",
    "--nnodes=1",
    "--nproc_per_node=1",
    "--master_port", "29513",  # ä½¿ç”¨æ–°çš„ç«¯å£é¿å…å†²çª
    "llava/train/train_mem.py",
    "--deepspeed", "scripts/zero3.json",
    "--model_name_or_path", base_model,
    "--chat_template", "qwen2",
    "--data_mixture", data_mixture,
    "--vision_tower", "Efficient-Large-Model/paligemma-siglip-so400m-patch14-448",
    "--depth_tower", "Efficient-Large-Model/paligemma-siglip-so400m-patch14-448",
    "--mm_vision_select_feature", "cls_patch",
    "--mm_projector", "mlp_downsample_3x3_fix",
    "--depth_projector", "mlp_downsample_3x3_fix",
    "--enable_depth", "False",  # æ‹¼æ¥å›¾ä¸ä½¿ç”¨depth
    "--use_depth_tower", "False",
    "--tune_vision_tower", "True",
    "--tune_mm_projector", "True",
    "--tune_language_model", "True",
    "--tune_depth_tower", "False",
    "--tune_depth_projector", "False",
    "--mm_vision_select_layer", "-2",
    "--mm_use_im_start_end", "False",
    "--mm_use_im_patch_token", "False",
    "--image_aspect_ratio", "dynamic",
    "--bf16", "True",
    "--output_dir", f"{output_dir}/model",
    "--num_train_epochs", "2",
    "--per_device_train_batch_size", "1",
    "--gradient_accumulation_steps", "4",
    "--evaluation_strategy", "no",
    "--save_strategy", "steps",
    "--save_steps", "500",
    "--save_total_limit", "3",
    "--learning_rate", "1e-5",
    "--weight_decay", "0.",
    "--warmup_ratio", "0.03",
    "--lr_scheduler_type", "cosine",
    "--logging_steps", "10",
    "--model_max_length", "16384",
    "--gradient_checkpointing", "True",
    "--dataloader_num_workers", "4",
    "--report_to", "none"
]

print("ğŸš€ æ‰§è¡Œè®­ç»ƒå‘½ä»¤:")
print(" ".join(cmd))
print("\n" + "="*70 + "\n")

# ==================== è¿è¡Œè®­ç»ƒ ====================
try:
    subprocess.run(cmd, check=True)
    print("\n" + "="*70)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {output_dir}/model")
    print("="*70)
except subprocess.CalledProcessError as e:
    print(f"\nâŒ è®­ç»ƒå‡ºé”™ (exit code: {e.returncode})")
    sys.exit(1)
except KeyboardInterrupt:
    print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    sys.exit(0)
